---
title: "Pre-registration for Group [insert group number]"
author: "1234567, 2345678, 3456789, 4567890, 09887653, 9876543" #replace with GUIDS 
output: word_document
---

#	1. What are the main hypotheses being tested in this study? Provide a concise rationale.

# 2. Describe the key variables specifying how they will be measured, how many levels or they have and how participants will be assigned (if relevant).

# 3. Describe your precise rule(s) for excluding observations and/or participants.

# 4. Describe exactly which inferential analyses you will conduct to examine the main hypotheses, including details of any assumption tests.

# 5. How many observations will be collected or what will determine sample size/statistical power? 

# References


# Analysis code

This template assumes that you will be running one t-test and a correlation, if you have decided to do something more complex then this template may not fit your needs. Some of the code has been completed for you to clean up the raw questionnaire output from Experimentum. Remember to knit the file after each step, it will make it easy to spot if you have made an error.

Finally, remember that the pilot data is a small sample compared to the larger dataset that you will work with for the full quant report There may be missing data or types of participants in the full data set that aren't present in this sample.

**You can delete the above instructions before you knit and submit your final pre-reg**

#### 1. Load in packages and data

```{r}
library(tidyverse)

# you will need to add extra packages in here to do the rest of your analyses

# run the below code without changing anything, it will help clean up some of the experimentum data. don't worry if you get a message about introduing NAs by coercion. 

dat <- read_csv("pilot_data.csv") %>%
  filter(user_status %in% c("guest", "registered", "student")) %>%
  group_by(user_id, q_id) %>%
  filter(row_number() == 1) %>%
  ungroup()

demographics <-  dat %>%
  filter(quest_name == "demographics") %>%
  select(user_id, q_name, dv, covid) %>%
  pivot_wider(names_from = "q_name", values_from = "dv")%>%
  select(-"NA") %>%
  mutate(employment = as.numeric(employment))

mslq <-  dat %>%
  filter(quest_name == "Full MSLQ") %>%
  select(user_id, q_name, dv) %>%
  mutate(dv = as.numeric(dv)) %>%
  pivot_wider(names_from = "q_name", values_from = "dv") 
```


#### 2. Join together the data files by their common columns using `inner_join()`, this will reduce the number of participants to only those that filled out both sections of the survey

```{r}

```

#### 3. Use select to retain only the variables you need for your chosen research design (including the user ID).

```{r}

```

#### 4. If necessary, use filter to retain only the observations you need, for example, you might need to delete participants above a certain age, or only use mature students etc.

```{r}

```

#### 5. Use `summary` or `str` to check what type of variable each variable is. Recode any necessary variables as factors and, if you would like to, change numeric codes (e.g., 1 for native speaker) into words to make it easier to read the output. 

```{r}

```

#### 6. Calculate the mean score for each participant for each sub-scale. There are a few ways you can do this but helpfully the Experimentum documentation provides example code to make this easier, you just need to adapt it for the variables you need. You may also want to change the `na.rm = TRUE` for the calculation of means depending on whether you want to only include participants who completed all questions (it will currently return an average even if there is missing data, you might not want this).

At the top of the code chunk below, change `eval = FALSE` to `eval = TRUE` once you have amended your code. The reason it is currently set to FALSE is to allow the file to knit.


```{r eval = FALSE}
dat_means <- data %>% # change data to the name of the data object you want to work from
  gather(var, val, question_1:question_5) %>% # change question_1:question_5 to select the questions for your 1st sub-scale 
  group_by_at(vars(-val, -var)) %>% # don't change this 
  summarise(scale1_mean = mean(val, na.rm = TRUE)) %>% # change scale1_mean to the name of your 1st sub-scale
  ungroup() %>% # always ungroup! 
  gather(var, val, question_1:question_5) %>% # change question_1:question_5 to select the questions for your 2nd scale
  group_by_at(vars(-val, -var)) %>% 
  summarise(scale2_mean = mean(val, na.rm = TRUE)) %>% # does not return sums with missing items 
  ungroup() 
```

#### 7. Now you have the dataset in the format that you need for analysis (you could actually combine all of the above steps together in one mega pipe-line of code, but only do that if you're feeling confident). Next, you should visualise the data for each analysis.

T-test visualisation

```{r}

```

Correlation visualisation

```{r}

```


#### 8. Now you should check that the data meets the assumptions of the tests you want to conduct.

T-test assumptions

```{r}

```

Correlation assumptions

```{r}

```


#### 9. You can now conduct your statistical analyses. Don't forget to calculate effect sizes for the t-tests!

T-test analysis

```{r}

```

Correlation analysis


```{r}

```


#### 10. Finally, you can use this chunk to conduct your power analysis for yotur minimum sample size.

```{r}

```

